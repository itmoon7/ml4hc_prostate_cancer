{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "os.chdir(\"/Users/ashwin/Dropbox (MIT)/MGH Prostate Research Group/RPDR/\")\n",
    "#os.chdir(\"/Users/dexinli/Dropbox (MIT)/MGH Prostate Research Group/RPDR/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import and Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv('Processed data/merged_data/df_merged_rp_positive_based_E.csv')\n",
    "merged_df.rename(columns={'Unnamed: 0': 'EMPI'}, inplace = True)\n",
    "print(len(merged_df.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# import other datasets when looking at comorbidity and diabetes features\n",
    "# ultimately, we did not decide to include these in the final prediction analysis\n",
    "data_merged_comorb = pd.read_csv('Processed data/merged_data/df_merged_rp_positive_based_C.csv')\n",
    "data_merged_comorb.set_index('Unnamed: 0', inplace = True)\n",
    "data_merged_comorb.index.name = 'EMPI'\n",
    "print(len(data_merged_comorb.index))\n",
    "\n",
    "empis_oi = set(data_merged.index) & set(data_merged_comorb.index)\n",
    "comorbs_oi = ['wscore_agg','diab_agg']\n",
    "data_merged2 = pd.concat([data_merged.loc[empis_oi], data_merged_comorb.loc[empis_oi][comorbs_oi]], axis = 1)\n",
    "data_merged2.reset_index(inplace=True)\n",
    "\n",
    "print(len(data_merged2.index))\n",
    "data_merged2.head()\n",
    "\n",
    "# Change first column to Unnamed: 0\n",
    "merged_df = merged_df.rename(columns={\"Unnamed: 0\": \"EMPI\"})\n",
    "\n",
    "# merge data_merged2 back onto data_merged; keep outer, make all the NAs 0\n",
    "# only keep EMPI, wscore_agg, diab_agg\n",
    "data_merged2 = data_merged2[[\"EMPI\", \"wscore_agg\", \"diab_agg\"]]\n",
    "merged_df = data_merged.merge(data_merged2, on=\"EMPI\", how=\"outer\")\n",
    "print(len(merged_df.index))\n",
    "merged_df.head()\n",
    "\n",
    "# Make all the NAs 0; we are assuming people with no comorbidities have comorbidity 0 here\n",
    "# Not an ideal assumption, which is why we ultimately did not go this route\n",
    "merged_df[\"wscore_agg\"] = merged_df[\"wscore_agg\"].fillna(0)\n",
    "merged_df[\"diab_agg\"] = merged_df[\"diab_agg\"].fillna(0)\n",
    "\n",
    "# drop any extra rows with NAs (those only have comorb data)\n",
    "merged_df = merged_df.dropna()\n",
    "print(len(merged_df.index))\n",
    "merged_df.head()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the outcome data\n",
    "outcome_df = pd.read_csv(\"Processed data/merged_data/df_outcome_rp_positive_final.csv\")\n",
    "outcome_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset outcome_df so it has the same number of observations as merged_df\n",
    "x_empi = merged_df[[\"EMPI\"]]\n",
    "y_df = outcome_df.merge(x_empi, on=\"EMPI\")\n",
    "\n",
    "print(len(y_df.index)) \n",
    "y_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We only look at bcr_ind right now (can look at 5-year, 10-year later)\n",
    "y_df = y_df[[\"EMPI\", \"bcr_ind\"]]\n",
    "y_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset merged_df so it also has the same number of observations as outcome_df\n",
    "y_empi = outcome_df[[\"EMPI\"]]\n",
    "x_df = merged_df.merge(y_empi, on=\"EMPI\")\n",
    "\n",
    "print(len(x_df.index))\n",
    "x_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort both columns based on EMPI\n",
    "x_df = x_df.sort_values(by=['EMPI'])\n",
    "y_df = y_df.sort_values(by=['EMPI'])\n",
    "\n",
    "x_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Change pT_stage_combined into a numerical feature\n",
    "print(x_df[\"pT_stage_combined\"].unique())\n",
    "\n",
    "x_df[\"pt1\"] = np.where((x_df[\"pT_stage_combined\"] == \"pt1\"), 1, 0)\n",
    "x_df[\"pt1a\"] = np.where((x_df[\"pT_stage_combined\"] == \"pt1a\"), 1, 0)\n",
    "x_df[\"pt1b\"] = np.where((x_df[\"pT_stage_combined\"] == \"pt1b\"), 1, 0)\n",
    "x_df[\"pt2\"] = np.where((x_df[\"pT_stage_combined\"] == \"pt2\") | (x_df[\"pT_stage_combined\"] == \"pt2a\") |\n",
    "                    (x_df[\"pT_stage_combined\"] == \"pt2b\") | (x_df[\"pT_stage_combined\"] == \"pt2c\"), 1, 0)\n",
    "x_df[\"pt3\"] = np.where((x_df[\"pT_stage_combined\"] == \"pt3\"), 1, 0)\n",
    "x_df[\"pt3a\"] = np.where((x_df[\"pT_stage_combined\"] == \"pt3a\"), 1, 0)\n",
    "x_df[\"pt3b\"] = np.where((x_df[\"pT_stage_combined\"] == \"pt3b\"), 1, 0)\n",
    "x_df[\"pt3c\"] = np.where((x_df[\"pT_stage_combined\"] == \"pt3c\"), 1, 0)\n",
    "\n",
    "# now remove the old pT_stage_combined columns\n",
    "x_df = x_df.drop(columns=[\"pT_stage_combined\"]) \n",
    "x_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split Data into Training, Testing, Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#  Create train, val, and test datasets using .6, .2, .2 split\n",
    "validation = 0.2\n",
    "test = 0.2\n",
    "\n",
    "X_train_, X_test_, Y_train_, Y_test_ = train_test_split(x_df, y_df, test_size = 1 - validation - test, random_state = 0)\n",
    "X_val_, X_test_, Y_val_, Y_test_ = train_test_split(X_test_, Y_test_, test_size = test/(test + validation), random_state = 0) \n",
    "\n",
    "X_test_.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Oversampling \n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "X_train, Y_train = oversample.fit_resample(X_train_, Y_train_['bcr_ind'])\n",
    "Y_test = Y_test_.set_index('EMPI')\n",
    "X_test = X_test_.set_index('EMPI')\n",
    "Y_val = Y_val_.set_index('EMPI')\n",
    "X_val = X_val_.set_index('EMPI')\n",
    "X_train = X_train.set_index('EMPI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this when NOT doing oversampling\n",
    "Y_test = Y_test_.set_index('EMPI')\n",
    "X_test = X_test_.set_index('EMPI')\n",
    "Y_val = Y_val_.set_index('EMPI')\n",
    "X_val = X_val_.set_index('EMPI')\n",
    "X_train = X_train_.set_index('EMPI')\n",
    "Y_train = Y_train_.set_index('EMPI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize training dataset to have zero mean and unit variance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_norm = scaler.fit_transform(X_train)\n",
    "\n",
    "# Normalize testing and validation data by transforming it via training dataset parameters\n",
    "X_val_norm = scaler.transform(X_val)\n",
    "X_test_norm = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train a logistic regression classifier on the training data while optimizing for best train accuracy\n",
    "#find the best hyperparameters among C=[0.1,0.25,0.5,1.] and penalty=[‘l1’,’l2’] on validation data.from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "for C_ in [.1, .25, .5, 1]:\n",
    "    for penalty_ in [\"l1\", \"l2\"]:\n",
    "        if penalty_ == \"l1\":\n",
    "            clf = LogisticRegression(penalty = penalty_, C = C_, solver =\"liblinear\", class_weight = 'balanced').fit(X_train_norm, Y_train)\n",
    "            print(\"C = \", C_, \"penalty =\", penalty_, clf.score(X_val_norm, Y_val))\n",
    "        else:\n",
    "            clf = LogisticRegression(penalty = penalty_, C = C_, class_weight = 'balanced').fit(X_train_norm, Y_train)\n",
    "            print(\"C = \", C_, \"penalty =\", penalty_, clf.score(X_val_norm, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Run XGBoosot, varying max_depth and min_child_weight\n",
    "for max_depth in [3, 6, 10]:\n",
    "    for min_child_weight in [1,5,10]:\n",
    "        model = XGBClassifier(max_depth = max_depth, min_child_weight = min_child_weight).fit(X_train_norm, Y_train)\n",
    "        print(\"max depth = \", max_depth, \"min child weight =\", min_child_weight, model.score(X_val_norm, Y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No oversampling, afterwards we retrain model on the training and validation datasets combined\n",
    "# turns into a 80-20 split\n",
    "X_train_, X_test_, Y_train_, Y_test_ = train_test_split(x_df, y_df, test_size = 0.2, random_state = 0)\n",
    "Y_test = Y_test_.set_index('EMPI')\n",
    "X_test = X_test_.set_index('EMPI')\n",
    "X_train = X_train_.set_index('EMPI')\n",
    "Y_train = Y_train_.set_index('EMPI')\n",
    "scaler = StandardScaler()\n",
    "X_train_norm = scaler.fit_transform(X_train)\n",
    "\n",
    "# Normalize testing and validation data by transforming it via training dataset parameters\n",
    "X_test_norm = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Oversampling, afterwards we retrain model on the training and validation datasets combined\n",
    "# turns into a 80-20 split\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "oversample = RandomOverSampler(sampling_strategy='minority')\n",
    "X_train_, X_test_, Y_train_, Y_test_ = train_test_split(x_df, y_df, test_size = 0.2, random_state = 0)\n",
    "X_train, Y_train = oversample.fit_resample(X_train_, Y_train_['bcr_ind'])\n",
    "Y_test = Y_test_.set_index('EMPI')\n",
    "X_test = X_test_.set_index('EMPI')\n",
    "X_train = X_train.set_index('EMPI')\n",
    "scaler = StandardScaler()\n",
    "X_train_norm = scaler.fit_transform(X_train)\n",
    "\n",
    "# Normalize testing and validation data by transforming it via training dataset parameters\n",
    "X_test_norm = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we run clf_best\n",
    "clf_best = LogisticRegression(penalty = \"l1\", C = .1, solver =\"liblinear\").fit(X_train_norm, Y_train)\n",
    "\n",
    "# Check for accuracy on test data\n",
    "print(clf_best.score(X_test_norm, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_best = XGBClassifier(max_depth = 10, min_child_weight = 1).fit(X_train_norm, Y_train)\n",
    "\n",
    "# Check for accuracy on test data\n",
    "print(model_best.score(X_test_norm, Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the AOC on the best Logistic Regression model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(Y_test, clf_best.predict_proba(X_test_norm)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the AOC on the best XGBoost Model\n",
    "from sklearn.metrics import roc_auc_score\n",
    "roc_auc_score(Y_test, model_best.predict_proba(X_test_norm)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make confusion matrix of predictions vs reality for best logistic regression model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "test_ypred = np.round(clf_best.predict_proba(X_test_norm)[:, 1])\n",
    "\n",
    "confusion_matrix(Y_test, test_ypred) # seems like we just don't have enough data points at all,\n",
    "# our model just predicts everything as no recurrence, because recurrence is so unlikely in our dataset anyway"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make confusion matrix of predictions vs reality for best XGBoost model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "test_ypred = np.round(model_best.predict_proba(X_test_norm)[:, 1])\n",
    "\n",
    "confusion_matrix(Y_test, test_ypred) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve for best logistic regression model\n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn import datasets, metrics, model_selection, svm\n",
    "\n",
    "metrics.plot_roc_curve(clf_best, X_test_norm, Y_test)  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ROC curve for best XGBoost model\n",
    "import matplotlib.pyplot as plt  \n",
    "from sklearn import datasets, metrics, model_selection, svm\n",
    "\n",
    "metrics.plot_roc_curve(model_best, X_test_norm, Y_test)  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate f1 score\n",
    "from sklearn.metrics import f1_score\n",
    "f1_score(Y_test, test_ypred) # low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal Threshold Tuning\n",
    "# search thresholds for imbalanced classification for best linear regression model\n",
    "from numpy import arange\n",
    "from numpy import argmax\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# apply threshold to positive probabilities to create labels\n",
    "def to_labels(pos_probs, threshold):\n",
    "\treturn (pos_probs >= threshold).astype('int')\n",
    " \n",
    "yhat = clf_best.predict_proba(X_test_norm)\n",
    "\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = yhat[:, 1]\n",
    "\n",
    "# define thresholds\n",
    "thresholds = arange(0, 1, 0.001)\n",
    "\n",
    "# evaluate each threshold\n",
    "scores = [f1_score(Y_test, to_labels(probs, t)) for t in thresholds]\n",
    "# get best threshold\n",
    "ix_best = argmax(scores)\n",
    "print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix_best], scores[ix_best]))\n",
    "print(confusion_matrix(Y_test, to_labels(probs, thresholds[ix_best])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal Threshold for Precision-Recall Curve\n",
    "# pr curve for logistic regression model\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# calculate pr-curve\n",
    "precision, recall, thresholds = precision_recall_curve(Y_test, probs)\n",
    "\n",
    "precision = precision[:-1]\n",
    "recall = recall[:-1]\n",
    "\n",
    "# plot the roc curve for the model\n",
    "pyplot.plot(thresholds, precision, marker='.', label='Precision')\n",
    "pyplot.plot(thresholds, recall, marker='.', label='Recall')\n",
    "\n",
    "# axis labels\n",
    "pyplot.xlabel('Threshold')\n",
    "pyplot.ylabel('Precision/Recall')\n",
    "pyplot.legend()\n",
    "\n",
    "# show the plot\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Performance on Different Hospitals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import hospital data from diagnoses files\n",
    "diagnosis = pd.read_csv('Raw Data/First/txt/KS185_20200918_114153_Dia.txt', sep=\"|\", header=0, low_memory=False)\n",
    "diagnosis_second = pd.read_csv('Raw Data/Second/txt/KS185_20200918_114153_Dia.txt', sep=\"|\", header=0, low_memory=False)\n",
    "diagnosis_third = pd.read_csv('Raw Data/Third/txt/KS185_20200918_114153_Dia.txt', sep=\"|\", header=0, low_memory=False)\n",
    "diagnosis_fourth = pd.read_csv('Raw Data/Fourth/txt/KS185_20200918_114153_Dia.txt', sep=\"|\", header=0, low_memory=False)\n",
    "diamerged = diagnosis.append([diagnosis_second, diagnosis_third, diagnosis_fourth], sort=True)\n",
    "diamerged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we just want EMPI and hospital columns\n",
    "# drop duplicates\n",
    "diamerged2 = diamerged[[\"EMPI\", \"Hospital\"]]\n",
    "print(len(diamerged2.index))\n",
    "diamerged2 = diamerged2.drop_duplicates(subset=\"EMPI\")\n",
    "print(len(diamerged2.index))\n",
    "print(diamerged2[\"Hospital\"].unique())\n",
    "print(diamerged2[\"Hospital\"].value_counts())\n",
    "diamerged2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Hospital data onto test data\n",
    "print(len(X_test_))\n",
    "print(len(Y_test_))\n",
    "\n",
    "X_test_hospital = X_test_.merge(diamerged2, on=\"EMPI\")\n",
    "Y_test_hospital = Y_test_.merge(diamerged2, on=\"EMPI\")\n",
    "\n",
    "print(len(X_test_hospital.index))\n",
    "print(len(Y_test_hospital.index))\n",
    "X_test_hospital.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we split the test datasets into smaller datasets based on hospital - compare MGH and BWH, two biggest hospitals\n",
    "# take the indices of mgh in X data, then subset those indices from Y dataset as well\n",
    "\n",
    "# mgh\n",
    "mgh_indices = X_test_hospital.index[X_test_hospital[\"Hospital\"] == \"MGH\"].tolist()\n",
    "X_test_mgh = X_test_hospital.loc[mgh_indices]\n",
    "# now we remove Hospital\n",
    "X_test_mgh = X_test_mgh.drop([\"EMPI\", \"Hospital\"], axis=1)\n",
    "\n",
    "X_test_mgh_norm = scaler.transform(X_test_mgh)\n",
    "Y_test_mgh = Y_test_hospital.loc[mgh_indices]\n",
    "Y_test_mgh = Y_test_mgh.drop([\"EMPI\", \"Hospital\"], axis=1)\n",
    "\n",
    "# bwh\n",
    "bwh_indices = X_test_hospital.index[X_test_hospital[\"Hospital\"] == \"BWH\"].tolist()\n",
    "X_test_bwh = X_test_hospital.loc[bwh_indices]\n",
    "X_test_bwh = X_test_bwh.drop([\"EMPI\", \"Hospital\"], axis=1)\n",
    "\n",
    "X_test_bwh_norm = scaler.transform(X_test_bwh)\n",
    "Y_test_bwh = Y_test_hospital.loc[bwh_indices]\n",
    "Y_test_bwh = Y_test_bwh.drop([\"EMPI\", \"Hospital\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check size of datasets\n",
    "print(\"mgh\", len(X_test_mgh.index))\n",
    "print(\"bwh\", len(X_test_bwh.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for accuracy on test data for each population\n",
    "print(clf_best.score(X_test_mgh_norm, Y_test_mgh))\n",
    "print(clf_best.score(X_test_bwh_norm, Y_test_bwh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predicted y test values for each population\n",
    "test_ypred_mgh = np.round(clf_best.predict_proba(X_test_mgh_norm)[:, 1])\n",
    "test_ypred_bwh = np.round(clf_best.predict_proba(X_test_bwh_norm)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the F1 score\n",
    "print(\"mgh\", f1_score(Y_test_mgh, test_ypred_mgh))\n",
    "print(\"bwh\", f1_score(Y_test_bwh, test_ypred_bwh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal Threshold Tuning for MGH\n",
    "# search thresholds for imbalanced classification on best overall population linear regression model\n",
    "from numpy import arange\n",
    "from numpy import argmax\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# apply threshold to positive probabilities to create labels\n",
    "def to_labels(pos_probs, threshold):\n",
    "\treturn (pos_probs >= threshold).astype('int')\n",
    " \n",
    "yhat_mgh = clf_best.predict_proba(X_test_mgh_norm)\n",
    "\n",
    "# keep probabilities for the positive outcome only\n",
    "probs_mgh = yhat_mgh[:, 1]\n",
    "\n",
    "# define thresholds\n",
    "thresholds = arange(0, 1, 0.001)\n",
    "\n",
    "# evaluate each threshold\n",
    "scores = [f1_score(Y_test_mgh, to_labels(probs_mgh, t)) for t in thresholds]\n",
    "\n",
    "# use the best threshold for all populations\n",
    "print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix_best], scores[ix_best]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal Threshold Tuning for BWH\n",
    "# search thresholds for imbalanced classification on best overall population linear regression model\n",
    "from numpy import arange\n",
    "from numpy import argmax\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# apply threshold to positive probabilities to create labels\n",
    "def to_labels(pos_probs, threshold):\n",
    "\treturn (pos_probs >= threshold).astype('int')\n",
    " \n",
    "yhat = clf_best.predict_proba(X_test_bwh_norm)\n",
    "\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = yhat[:, 1]\n",
    "\n",
    "# define thresholds\n",
    "thresholds = arange(0, 1, 0.001)\n",
    "\n",
    "# evaluate each threshold\n",
    "scores = [f1_score(Y_test_bwh, to_labels(probs, t)) for t in thresholds]\n",
    "\n",
    "# get best threshold\n",
    "print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix_best], scores[ix_best]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Performance on Different Races"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we split the test datasets into smaller datasets based on race\n",
    "# take the indices of race in X data, then subset those indices from Y dataset as well\n",
    "\n",
    "# white\n",
    "white_indices = X_test.index[X_test[\"White\"] == 1].tolist()\n",
    "X_test_white = X_test.loc[white_indices]\n",
    "X_test_white_norm = scaler.transform(X_test_white)\n",
    "Y_test_white = Y_test.loc[white_indices]\n",
    "\n",
    "# black\n",
    "black_indices = X_test.index[X_test[\"Black\"] == 1].tolist()\n",
    "X_test_black = X_test.loc[black_indices]\n",
    "X_test_black_norm = scaler.transform(X_test_black)\n",
    "Y_test_black = Y_test.loc[black_indices]\n",
    "\n",
    "# asian\n",
    "asian_indices = X_test.index[X_test[\"Asian\"] == 1].tolist()\n",
    "X_test_asian = X_test.loc[asian_indices]\n",
    "X_test_asian_norm = scaler.transform(X_test_asian)\n",
    "Y_test_asian = Y_test.loc[asian_indices]\n",
    "\n",
    "# hispanic\n",
    "hispanic_indices = X_test.index[X_test[\"Hispanic\"] == 1].tolist()\n",
    "X_test_hispanic = X_test.loc[hispanic_indices]\n",
    "X_test_hispanic_norm = scaler.transform(X_test_hispanic)\n",
    "Y_test_hispanic = Y_test.loc[hispanic_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check size of datasets\n",
    "print(\"white\", len(X_test_white.index))\n",
    "print(\"black\", len(X_test_black.index))\n",
    "print(\"asian\", len(X_test_asian.index))\n",
    "print(\"hispanic\", len(X_test_hispanic.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for accuracy on test data for each population\n",
    "print(\"white\", clf_best.score(X_test_white_norm, Y_test_white))\n",
    "print(\"black\", clf_best.score(X_test_black_norm, Y_test_black))\n",
    "print(\"asian\", clf_best.score(X_test_asian_norm, Y_test_asian))\n",
    "print(\"hispanic\", clf_best.score(X_test_hispanic_norm, Y_test_hispanic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predicted y test values for each population\n",
    "test_ypred_white = np.round(clf_best.predict_proba(X_test_white_norm)[:, 1])\n",
    "test_ypred_black = np.round(clf_best.predict_proba(X_test_black_norm)[:, 1])\n",
    "test_ypred_asian = np.round(clf_best.predict_proba(X_test_asian_norm)[:, 1])\n",
    "test_ypred_hispanic = np.round(clf_best.predict_proba(X_test_hispanic_norm)[:, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the F1 score\n",
    "print(\"white\", f1_score(Y_test_white, test_ypred_white))\n",
    "print(\"black\", f1_score(Y_test_black, test_ypred_black))\n",
    "print(\"asian\", f1_score(Y_test_asian, test_ypred_asian))\n",
    "print(\"hispanic\", f1_score(Y_test_hispanic, test_ypred_hispanic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal Threshold Tuning for White\n",
    "# search thresholds for imbalanced classification on best overall population linear regression model\n",
    "from numpy import arange\n",
    "from numpy import argmax\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# apply threshold to positive probabilities to create labels\n",
    "def to_labels(pos_probs, threshold):\n",
    "\treturn (pos_probs >= threshold).astype('int')\n",
    " \n",
    "yhat = clf_best.predict_proba(X_test_white_norm)\n",
    "\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = yhat[:, 1]\n",
    "\n",
    "# define thresholds\n",
    "thresholds = arange(0, 1, 0.001)\n",
    "\n",
    "# evaluate each threshold\n",
    "scores = [f1_score(Y_test_white, to_labels(probs, t)) for t in thresholds]\n",
    "\n",
    "# get best threshold\n",
    "print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix_best], scores[ix_best]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal Threshold Tuning for black\n",
    "# search thresholds for imbalanced classification on best overall population linear regression model\n",
    "from numpy import arange\n",
    "from numpy import argmax\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# apply threshold to positive probabilities to create labels\n",
    "def to_labels(pos_probs, threshold):\n",
    "\treturn (pos_probs >= threshold).astype('int')\n",
    " \n",
    "yhat = clf_best.predict_proba(X_test_black_norm)\n",
    "\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = yhat[:, 1]\n",
    "\n",
    "# define thresholds\n",
    "thresholds = arange(0, 1, 0.001)\n",
    "\n",
    "# evaluate each threshold\n",
    "scores = [f1_score(Y_test_black, to_labels(probs, t)) for t in thresholds]\n",
    "\n",
    "# get best threshold\n",
    "print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix_best], scores[ix_best]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal Threshold Tuning for asian\n",
    "# search thresholds for imbalanced classification on best overall population linear regression model\n",
    "from numpy import arange\n",
    "from numpy import argmax\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# apply threshold to positive probabilities to create labels\n",
    "def to_labels(pos_probs, threshold):\n",
    "\treturn (pos_probs >= threshold).astype('int')\n",
    " \n",
    "yhat = clf_best.predict_proba(X_test_asian_norm)\n",
    "\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = yhat[:, 1]\n",
    "\n",
    "# define thresholds\n",
    "thresholds = arange(0, 1, 0.001)\n",
    "\n",
    "# evaluate each threshold\n",
    "scores = [f1_score(Y_test_asian, to_labels(probs, t)) for t in thresholds]\n",
    "\n",
    "# get best threshold\n",
    "print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix_best], scores[ix_best]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimal Threshold Tuning for hispanic\n",
    "# search thresholds for imbalanced classification on best overall population linear regression model\n",
    "from numpy import arange\n",
    "from numpy import argmax\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# apply threshold to positive probabilities to create labels\n",
    "def to_labels(pos_probs, threshold):\n",
    "\treturn (pos_probs >= threshold).astype('int')\n",
    " \n",
    "yhat = clf_best.predict_proba(X_test_hispanic_norm)\n",
    "\n",
    "# keep probabilities for the positive outcome only\n",
    "probs = yhat[:, 1]\n",
    "\n",
    "# define thresholds\n",
    "thresholds = arange(0, 1, 0.001)\n",
    "\n",
    "# evaluate each threshold\n",
    "scores = [f1_score(Y_test_hispanic, to_labels(probs, t)) for t in thresholds]\n",
    "\n",
    "# get best threshold\n",
    "print('Threshold=%.3f, F-Score=%.5f' % (thresholds[ix_best], scores[ix_best]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demographics in Training and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of patients in test set\n",
    "len(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of patients in training set\n",
    "len(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding mean and confidence intervals by race in training set\n",
    "import math\n",
    "for var in ['White', 'Black', 'Hispanic', 'Asian']:\n",
    "    mean = X_train[var].mean()\n",
    "    std = X_train[var].std()\n",
    "    print(str(mean-1.96*std/math.sqrt(len(X_train[var]))) + \",\" + str(mean+1.96*std/math.sqrt(len(X_train[var]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding mean and confidence intervals by race in test set\n",
    "import math\n",
    "for var in ['White', 'Black', 'Hispanic', 'Asian']:\n",
    "    mean = X_test[var].mean()\n",
    "    std = X_test[var].std()\n",
    "    print(str(mean-1.96*std/math.sqrt(len(X_test[var]))) + \",\" + str(mean+1.96*std/math.sqrt(len(X_test[var]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding mean and confidence intervals for different features in training set\n",
    "for var in ['overall_grade_group', 'Age at RP', 'psa_prior_to_rp', 'margin', 'pt2', 'pt3a', 'pt3b']:\n",
    "    mean = X_train[var].mean()\n",
    "    std = X_train[var].std()\n",
    "    print(str(mean-1.96*std/math.sqrt(len(X_train[var]))) + \",\" + str(mean+1.96*std/math.sqrt(len(X_train[var]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding mean and confidence intervals for different features in test set\n",
    "for var in ['overall_grade_group', 'Age at RP', 'psa_prior_to_rp', 'margin', 'pt2', 'pt3a', 'pt3b']:\n",
    "    mean = X_test[var].mean()\n",
    "    std = X_test[var].std()\n",
    "    print(str(mean-1.96*std/math.sqrt(len(X_test[var]))) + \",\" + str(mean+1.96*std/math.sqrt(len(X_test[var]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding distribution of grades in training set\n",
    "for var in range(1,6,1):\n",
    "    temp = X_train['overall_grade_group'] == var\n",
    "    mean = temp.mean()\n",
    "    std = temp.std()\n",
    "    print(str(mean-1.96*std/math.sqrt(len(temp))) + \",\" + str(mean+1.96*std/math.sqrt(len(temp))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding distribution of grades in test set\n",
    "for var in range(1,6,1):\n",
    "    temp = X_test['overall_grade_group'] == var\n",
    "    mean = temp.mean()\n",
    "    std = temp.std()\n",
    "    print(str(mean-1.96*std/math.sqrt(len(temp))) + \",\" + str(mean+1.96*std/math.sqrt(len(temp))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering out rows with null diabetes value in training\n",
    "X_train = X_train[~X_train['diab_agg'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean and confidence intervals for diabetes in training set\n",
    "mean = X_train['diab_agg'].mean()\n",
    "std = X_train['diab_agg'].std()\n",
    "print(mean)\n",
    "print(str(mean-1.96*std/math.sqrt(len(X_train['diab_agg']))) + \",\" + str(mean+1.96*std/math.sqrt(len(X_train['diab_agg']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering out rows with null diabetes value in test\n",
    "X_test = X_test[~X_test['diab_agg'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean and confidence intervals for diabetes in training set\n",
    "mean = X_test['diab_agg'].mean()\n",
    "std = X_test['diab_agg'].std()\n",
    "print(mean)\n",
    "print(str(mean-1.96*std/math.sqrt(len(X_test['diab_agg']))) + \",\" + str(mean+1.96*std/math.sqrt(len(X_test['diab_agg']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
